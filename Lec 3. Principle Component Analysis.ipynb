{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 5. Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Classifier works well on the training set. $\\Leftarrow$ ERM (Empirical Risk Minimization);\n",
    "\n",
    "2. Hypothesis Space is not so long;\n",
    "\n",
    "3. Filter out noise and __extract__ the __relevant information__ from the given data set. __Decrease__ both __noise__ and __redundancy(correlated data)__ in the data set;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FACE Recognization\n",
    "\n",
    "|1|Insupervised|PCA (Principle Component Analysis)\n",
    "|:-|:-|:-\n",
    "|2|Supervised|LDA (Linear Discriminant Analysis)\n",
    "|3|Kernal Methods|Kernal Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 What is PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __P__rincipal __C__omponent __A__nalysis (PCA) is a __statistical procedure__ that uses an __orthogonal transformation__ to convert a set of __observations of possibly correlated variables__ into a set of values of __linearly uncorrelated variables__ called __principal components__. \n",
    "\n",
    "\n",
    "- __Purpose__: The goal of principal component analysis is to identify the most __meaningful basis__ to __re-express__ a data set. The hope is that this __new basis__ will __filter out__ the __noise__ and __reveal hidden structure__.\n",
    "\n",
    "\n",
    "\n",
    "- The __number__ of __p__rincipal __c__omponents __<=__ to the __number__ of __original variables__. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Assumption of PCA\n",
    "\n",
    "1. __Linearity__\n",
    "    - Assumes the data set to be linear combinations of the variables;\n",
    "    \n",
    "2. The importance of __mean__ and __covariance__\n",
    "    - There is no guarantee that the directions of maximum variance will contain good features for discrimination;\n",
    "    \n",
    "3. That __large variances__ have __important__ dynamics\n",
    "    - Assumes that components with larger variance correspond to interesting dynamics and lower ones correspond to noise.\n",
    "    \n",
    "__Note__:\n",
    "\n",
    "Where regression determines a line of best fit to a data set, factor analysis determines several orthogonal lines of best fit to the data set.\n",
    "\n",
    "The orthogonality of  principal components implies that PCA finds the __most uncorrelated__ components to explain as __much variation__ in the data as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 How PCA Transformation Defined?\n",
    "- This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. \n",
    "\n",
    "\n",
    "- The resulting vectors are an uncorrelated orthogonal basis set. \n",
    "\n",
    "\n",
    "- PCA is sensitive to the relative scaling of the original variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Where to use?\n",
    "\n",
    "1) Image Compression;\n",
    "\n",
    "2) Eigen Face;\n",
    "\n",
    "3) Finding Patterns in data of high dimension;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Mathmatics\n",
    "\n",
    "1) Standard deviation;\n",
    "\n",
    "2) Covariance;\n",
    "\n",
    "3) Eigen Vectors;\n",
    "\n",
    "4) Eigen Values;\n",
    "\n",
    "5) Orthogonal matrix;\n",
    "\n",
    "6) __Spectral Therom__: A __matrix__ is __symmetric__ if and only if it is __orthogonally diagonalizable__;\n",
    "\n",
    "- A __symmetric matrix__ __A__ can be written as $A = EDE^T$, where __D__ is a __diagnol matrix__, __E__ is a matrix of A's __eigen vectors__;\n",
    "\n",
    "- [Gram–Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process)\n",
    "\n",
    "- A symmetric matrix is diagonalized by a matrix of its orthonormal eigenvectors.\n",
    "\n",
    "7) __singular value decomposition__ (SVD): __PCA__ is intimately related to the mathematical technique of __singular value decomposition__ (SVD);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definition of Principle Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input__: $X_i\\in R^d,X=[X_1,X_2,...,X_N]\\in R^{\\space d\\times N}\\space,\\space i\\space =\\space 1,...,N$;\n",
    "\n",
    "__Output__: Projection Matrix, $w\\in R^{\\space d\\times d'} \\Rightarrow w^Tx\\in R^{\\space d'\\times N}\\space\\Rightarrow ww^Tx\\in R^{\\space d\\times N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Find:\n",
    "- The variance of d random variables and\n",
    "- Structure of the covariance or correclation between d variable\n",
    "\n",
    "If d is large, we want to focus on only some part of the d' variables while still preserve most of the information.\n",
    "\n",
    "### 1.2 Questions: \n",
    "    \n",
    "1. Which samples can we ignore?\n",
    "\n",
    "    1) dependent variables -> __high covariance__ \n",
    "    \n",
    "    2) constant\n",
    "    \n",
    "    3) __noise__ -> __low variance__\n",
    "    \n",
    "    4) constant + noise\n",
    "    \n",
    "2. Which samples do we want to keep?\n",
    "\n",
    "    1) __low covariance__ $\\approx$ uncorrelated, which means this sample doesn't depend on others too much.\n",
    "    \n",
    "    2) changes a lot ->  large variances have important dynamics\n",
    "        \n",
    "        a) high variance, can be treated as 'data'\n",
    "        \n",
    "        b) low variance can be treated as 'noise' \n",
    "## $\\text{signal - noise ration}: SNR=\\frac{\\sigma_{data}^2}{\\sigma_{noise}^2}$\n",
    "\n",
    "    So that, maximizes SNR = maximizes the variance between axis\n",
    "\n",
    "3. How we describe ‘most important’ features using math?\n",
    "\n",
    "    - Variance\n",
    "    \n",
    "4. How do we represent our data, so that the most important feature can be extracted easily?\n",
    "\n",
    "    - Changes of basis;\n",
    "\n",
    "5. What we are looking for?\n",
    "\n",
    "    Re-express sample $x$ as $w^Tx$, which decrease both noise and redundancy;\n",
    "    \n",
    "    Since we want $w^Tx$ has __minimized covariance__ and __maximized__ variance, the ideal covariance matrix of $w^Tx$ is __diagonalized__ matrix (with zeros in the off-diagonal terms).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Definition of Priciple Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:\n",
    "\n",
    "For linear function,\n",
    "\n",
    "$\\alpha_{1}^{T}X\\space=\\space \\alpha_{11}x_{1}+...+\\alpha_{1d'}x_{d'}=\\sum\\limits_{j=1}^{d'}\\alpha_{1j}x_{j}$\n",
    "\n",
    "where, $\\alpha_{1}^TX$ having maximum variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:\n",
    "\n",
    "For linear function $\\alpha_{2}^TX$, which is uncorrelated with $\\alpha_{1}^TX$\n",
    "\n",
    "$\\alpha_{2}^TX = \\alpha_{21}x_{1}+...+\\alpha_{2d'}x_{d'}=\\sum\\limits_{j=1}^{d'}\\alpha_{2j}x_{j}$\n",
    "\n",
    "where, $\\alpha_{2}^{T}X$ having maximum variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step k:\n",
    "\n",
    "$\\alpha_{k}^{T}X$ is the kth PC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $w = [\\alpha_1,\\alpha_2,...,\\alpha_k]$, so  \n",
    "    \n",
    "## \\begin{equation}\n",
    "w^T=\\begin{bmatrix}\n",
    "\\alpha_1^T \\\\\n",
    "\\alpha_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_k^T\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "and __w__ is __Orthogonal matrix__,i.e\n",
    "\n",
    "## $w^Tw=I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 How to Find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance Matrix of X: $\\Sigma$.(If $\\Sigma$ is unkown, replace it by a sample covariance matrix S), quantifies the correlation between all possible pairs of measurement.\n",
    "\n",
    "For kth PC: \n",
    "\n",
    "### $Y_k = \\alpha_k^TX$\n",
    "\n",
    "where, \n",
    "\n",
    "- ### $\\alpha_{k}$: is an eigenvector of $\\Sigma$ corresponding to its kth largest eigenvalue $\\lambda_{k}$;\n",
    "\n",
    "- ### Fortheremore, if $\\alpha_{k}^{T}\\alpha_{k}=1$(with vector) $\\Rightarrow$ $var[Y_{k}]=\\lambda_{k}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Derive -> How to get $\\alpha$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input__: $X_i\\in R^d,X=[X_1,X_2,...,X_N]\\in R^{\\space d\\times N}\\space,\\space i\\space =\\space 1,...,N$;\n",
    "\n",
    "__Output__: Projection Matrix, $w\\in R^{\\space d\\times d'} \\Rightarrow w^Tx\\in R^{\\space d'\\times N}\\space\\Rightarrow ww^Tx\\in R^{\\space d\\times N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1  Find 1st Principle Component\n",
    "\n",
    "__Proof__:\n",
    "\n",
    "###    \\begin{equation}\n",
    "    max\\space var[\\alpha_1^TX]=\\alpha_1^T\\Sigma\\alpha_1 \\\\\n",
    "    st. \\alpha_1^T\\alpha_1 = 1, \\sum_{i=1}^{d'}\\alpha_{1i}^2=1 \\\\\n",
    "    \\overset{Lagrange}{\\Rightarrow} max \\space \\alpha_1^T\\Sigma\\alpha_1 - \\lambda(\\alpha_1^T\\alpha_1-1) \\\\\n",
    "    \\frac{d}{d\\alpha_1} \\alpha_1^T\\Sigma\\alpha_1-\\lambda(\\alpha_1^T\\alpha_1-1)=0 \\\\\n",
    "    \\Rightarrow 2\\Sigma\\alpha_1-2\\lambda\\alpha_1=0 \\\\\n",
    "    \\Rightarrow \\Sigma\\alpha_1-\\lambda\\alpha_1 = 0 \\\\\n",
    "    \\Rightarrow (\\Sigma - \\lambda I_{d'})\\alpha_1 = 0 \\\\\n",
    "    \\alpha_1 \\in R^{d'}, \\Sigma \\in R^{d'\\times d'}, \\lambda \\in R\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__:\n",
    "\n",
    "### \\begin{equation}\n",
    "Var(\\alpha_1^TX) = E\\{[\\alpha_1^TX-E(\\alpha_1^TX)][\\alpha_1^TX-E(\\alpha_1^TX)]^T\\} \\\\\n",
    "= E[\\alpha_1^TXX^T\\alpha_1-\\alpha_1^TXE(\\alpha_1^TX)-X^T\\alpha_1E(\\alpha_1^TX)+E(\\alpha_1^TX)^2]\\\\\n",
    "= \\alpha_1^TE(XX^T)\\alpha_1 - E(\\alpha_1^TX)^2 - E(X^T\\alpha_1)E(\\alpha_1^TX) + E(\\alpha_1^TX)^2\\\\\n",
    "= \\alpha_1^TE(XX^T)\\alpha_1 - E(X^T\\alpha_1)E(\\alpha_1^TX)\\\\\n",
    "= \\alpha_1^T\\Sigma\\alpha_1 \\text{(if the mean of X is zero)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\begin{equation}\n",
    "(\\Sigma - \\lambda I_p)\\alpha_1 = 0 \\\\\n",
    "1^{st}: \\lambda\\space \\text{is an eigenvalue of}\\space \\Sigma, and\\\\\n",
    "2^{nd}: \\alpha_1 \\text{is the corresponding eigenvector} \\\\\n",
    "\\Rightarrow max\\space Var(\\alpha_1^T X) = max\\space \\alpha_1^T\\Sigma\\alpha_1 \\\\\n",
    "= max\\space \\alpha_1^T\\lambda\\alpha_1, since\\space \\alpha_1^T\\alpha_1=1 \\\\\n",
    "3^{rd}: = max\\space \\lambda\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1) The Lagrangian $\\lambda$ is the eigenvalue of covariance matirx of x, $\\Sigma$;\n",
    "\n",
    "2) $\\alpha_1$ is the corresponding eigenvector;\n",
    "\n",
    "4) $\\Sigma$ is a symmetric matrix;\n",
    "\n",
    "3) Finding PC $\\iff$ Find the maximum eigenvalue of covariance matirx of x, $\\Sigma$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ must be as large as possible and $\\alpha_1$ is the eigenvector corresponding to the largest eigenvalue of $\\Sigma$;\n",
    "\n",
    "### \\begin{equation}\n",
    "\\Rightarrow \\lambda_1 = \\alpha_1^{*T}\\Sigma\\alpha_1^{*} = Var(\\alpha_1^{*T}X)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2  Find 2nd Principle Component\n",
    "\n",
    "### \\begin{equation}\n",
    "max\\space \\alpha_2^T\\Sigma\\alpha_2 \\\\\n",
    "st. \\alpha_2^T\\alpha_2 = 1, \\underbrace{cov[\\alpha_1^TX,\\alpha_2^TX]=0}_\\text{make PCs unrelated} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "__Proof__:\n",
    "### \\begin{equation}\n",
    "cov[\\alpha_1^TX,\\alpha_2^TX] \\\\\n",
    "= E[\\alpha_1^TX-E(\\alpha_1^TX)]E[\\alpha_2^TX-E(\\alpha_2^TX)]^T \\\\\n",
    "= \\alpha_1^TE[X-\\mu_X]E[X-\\mu_X]^T\\alpha_2 \\\\\n",
    "= \\alpha_1^T\\Sigma\\alpha_2 \\\\\n",
    "= \\alpha_2^T\\Sigma\\alpha_1 \\\\\n",
    "= \\alpha_2^T\\lambda_1\\alpha_1 \\\\\n",
    "= \\lambda_1\\alpha_2^T\\alpha_1 = 0 \\\\\n",
    "\\Rightarrow\n",
    "\\left \\{\n",
    "  \\begin{aligned}\n",
    "    &\\alpha_1^T\\Sigma\\alpha_2 = 0 && \\\\\n",
    "    &\\alpha_2^T\\Sigma\\alpha_1 = 0 && \\\\\n",
    "    &\\alpha_1^T\\alpha_1 = 0 && \\\\\n",
    "    &\\alpha_2^T\\alpha_1 = 0 && \\\\\n",
    "  \\end{aligned} \\right. \\\\\n",
    "\\overset{Lagrange}{\\Rightarrow} \\alpha_2^T\\Sigma\\alpha_2 - \\lambda(\\alpha_2^T\\alpha_2-1)-\\phi(\\alpha_2^T\\alpha_1)\\\\\n",
    "\\frac{d}{d\\alpha_2}[\\alpha_2^T\\Sigma\\alpha_2 - \\lambda(\\alpha_2^T\\alpha_2-1)-\\phi(\\alpha_2^T\\alpha_1)] = 0 \\\\\n",
    "\\Rightarrow 2\\Sigma\\alpha_2 - 2\\lambda\\alpha_2-\\phi\\alpha_1 = 0 \\\\\n",
    "\\Rightarrow \\Sigma\\alpha_2 - \\lambda\\alpha_2-\\phi\\alpha_1 = 0 \\\\\n",
    "\\Rightarrow \\underset{=0}{\\alpha_1^T\\Sigma\\alpha_2} - \\underset{=0}{\\alpha_1^T\\alpha_2} - \\underset{=1}{\\phi\\alpha_1^T\\alpha_1} = 0 \\\\\n",
    "\\Rightarrow \\phi = 0 \\\\\n",
    "\\Rightarrow (\\Sigma - \\lambda I_{d'})\\alpha_2 = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\begin{equation}\n",
    "(\\Sigma - \\lambda I_{d'})\\alpha_2 = 0 \\\\\n",
    "\\Rightarrow\\lambda\\space \\text{is an eigenvalue of}\\space \\Sigma, and\\\\\n",
    "\\alpha_2 \\text{is the corresponding eigenvector} \\\\\n",
    "\\Rightarrow max\\space Var(\\alpha_1^T X) = max\\space \\alpha_1^T\\Sigma\\alpha_1 \\\\\n",
    "= max\\space \\alpha_1^T\\lambda\\alpha_1, since\\space \\alpha_1^T\\alpha_1=1 \\\\\n",
    "= max\\space \\lambda\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1) The Lagrangian $\\lambda$ is the eigenvalue of covariance matirx of x, $\\Sigma$;\n",
    "\n",
    "2) $\\alpha_2$ is the corresponding eigenvector;\n",
    "\n",
    "3) Finding PC $\\iff$ Find the maximum eigenvalue of covariance matirx of x, $\\Sigma$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ must be as large as possible and $\\alpha_1$ is the eigenvector corresponding to the largest eigenvalue of $\\Sigma$;\n",
    "\n",
    "### \\begin{equation}\n",
    "\\Rightarrow \\lambda_2 = \\alpha_2^{*T}\\Sigma\\alpha_2^{*} = Var(\\alpha_2^{*T}X)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 General Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\begin{equation}\n",
    "\\lambda_k = \\alpha_k^{*T}\\Sigma\\alpha_k^{*} = Var(\\alpha_k^{*T}X)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we proved the statements in __1.2__, which is:\n",
    "\n",
    "### $Y_k = \\alpha_k^TX$\n",
    "\n",
    "where, \n",
    "\n",
    "- ### $\\alpha_{k}$: is an eigenvector of $\\Sigma$ corresponding to its kth largest eigenvalue $\\lambda_{k}$;\n",
    "\n",
    "- ### Fortheremore, if $\\alpha_{k}^{T}\\alpha_{k}=1$(with vector) $\\Rightarrow$ $var[Y_{k}]=\\lambda_{k}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means, to get w, we need to calcualte the eigen value of the covariance matrix of samples. Then, choose the highest d' and order them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solving PCA Using Eigenvector Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have\n",
    "\n",
    "__Input__: $X_i\\in R^d,X=[X_1,X_2,...,X_N]\\in R^{\\space d\\times N}\\space,\\space i\\space =\\space 1,...,N$;\n",
    "\n",
    "__Output__: Projection Matrix $Y = w^TX$, $w\\in R^{\\space d\\times d'} \\Rightarrow w^TX\\in R^{\\space d'\\times N}\\space\\Rightarrow ww^TX\\in R^{\\space d\\times N} (reconstruct)$\n",
    "\n",
    "## $Y = w^TX$\n",
    "\n",
    "Since we want __Y__ to be a __diagonalized matrix__,the next step is to check __Y__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Covariance Matrix of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\begin{equation}\n",
    "cov[Y] = cov[w^TX] = E[w^TX-E(w^TX)]E[w^TX-E(w^TX)]^T \\\\\n",
    "\\overset{\\text{since mean of X is zero}}{=} E[w^TX]E[w^TX]^T =w^TE[X]E[X]^Tw = w^T\\Sigma w\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "\n",
    "__Spectral Theorem__: A __matrix__ is __symmetric__ if and only if it is __orthogonally diagonalizable__.\n",
    "\n",
    "1) If A is __orthogonally diagonalizable__, then A is a __symmetric matrix__.\n",
    "\n",
    "Proof: \n",
    "## \\begin{equation}\n",
    "A = ED E^T \\\\\n",
    "A ^T = (ED E^T)^T = ED E^T \\\\\n",
    "\\Rightarrow A\\space \\text{is symmetric}\n",
    "\\end{equation}\n",
    "Q.E.D\n",
    "\n",
    "2) If A is a __symmetric matrix__, then A is __orthogonally diagonalizable__.\n",
    "\n",
    "Proof:\n",
    "\n",
    "By induction, for every $1\\times1$ matrix A: if A = [a], then A = [1][a][1] = UAU^T;\n",
    "\n",
    "Now, assume that\n",
    "\n",
    "(**) every (n-1)$\\times$(n-1) symmetric matrix is orthogonally diagonalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Spectral Theorem, since $\\Sigma$ is symmetric matrix, we can get:\n",
    "\n",
    "## \\begin{equation}\n",
    "\\Sigma = w \\lambda w^T \\\\\n",
    "\\Rightarrow cov[Y] = w^T \\Sigma w = w^T w\\lambda w^T w = (ww)^T\\lambda(ww) = \\lambda\n",
    "\\end{equation}\n",
    "\n",
    "- Which means __cov[Y]__ is diagonalized by w;\n",
    "\n",
    "- For covariance matrix of Y, all __off diagonal__ terms are __zero__, and all __diagona__l terms are the __eigen value__ of __cov[x]__, __x__ has __zero mean__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Process of Computing PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Subtracting off the mean of each measurement X;\n",
    "\n",
    "2) computing the eigenvectors of cov[x];\n",
    "\n",
    "3) Pick the first d' largest eigenvalue as w;\n",
    "\n",
    "4) Compute $w^T$;\n",
    "\n",
    "Now insteading of working on X, we can focus on $w^TX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1 Step1 -> Subtract the mean from each of the dimensions for X\n",
    "\n",
    "Substract the mean $\\rightarrow$ now all the data's mean = 0, in this way, the __covariance matrix__ of __X__ can be calculated as:\n",
    "\n",
    "## $cov[x] = E[X]E[X]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\begin{equation}\n",
    "\\bar{X}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}X_i\\space,\\bar{X_1}=\\frac{1}{N}X1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $for\\space i=1,...,N;\\space \\widetilde{X_i} = X_i-\\bar{X}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. [A tutorial on Principal Components Analysis 1](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)\n",
    "\n",
    "2. [A tutorial on Principal Components Analysis 2](https://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "3. [A Tutorial on Principal Component Analysis PPT](http://www.cvip.louisville.edu/wwwcvip/education/ECE523/PCA%20Tutorial%20-%20Sept%2009.pdf)\n",
    "\n",
    "4. [Principal Component Analysis](http://webspace.ship.edu/pgmarr/Geo441/Lectures/Lec%2017%20-%20Principal%20Component%20Analysis.pdf)\n",
    "\n",
    "5. [Are there implicit Gaussian assumptions in the use of PCA (principal components analysis)?](https://www.quora.com/Are-there-implicit-Gaussian-assumptions-in-the-use-of-PCA-principal-components-analysis)\n",
    "\n",
    "6. [Singular Value Decomposition](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)\n",
    "\n",
    "7. [Eigenvalue Problems](http://dept.stat.lsa.umich.edu/~kshedden/Courses/Stat606/Notes/eigen.pdf)\n",
    "\n",
    "8. [Orthogonally Diagonalizable Matrices](http://www.math.wustl.edu/~freiwald/309orthogdiag.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
