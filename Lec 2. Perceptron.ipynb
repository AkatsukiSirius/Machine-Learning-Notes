{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Seperability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y: set of all the possible labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{i}\\in R^{d},\\space y_{i}\\in \\{-1,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$find:\\space sgn(\\underset{weight\\space vector}{w^{T}}X_{i}+\\underset{bias\\space term}{b_{i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "w_{1},w_{2} \\cdots, w_{n}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1}\n",
    "\\\\\n",
    "x_{2}\n",
    "\\\\\n",
    "\\vdots\n",
    "\\\\\n",
    "x_{d}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "sgn(x)=\\begin{cases} 1,\\space x>0 \\\\ -1,\\space x<0 \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{y_{i}}=sgn(w_{i}^{T}X_{i})\n",
    "\\\\\n",
    "\\begin{cases}\n",
    "y_{i}=+1,\\space w^{T}X_{i}>0,\\space then\\space correct\n",
    "\\\\\n",
    "y_{i}=-1,\\space w^{T}X_{i}<0,\\space then\\space correct\n",
    "\\end{cases}\n",
    "\\\\\n",
    "\\Rightarrow y_{i}(w^{T}x_{i})>0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Def: \\exists w^{*},\\space \\forall \\space y_{i}(\\underset{Scalar}{\\underline{w^{*T}X_{i}}})>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Perceptron Algorithm](https://www.cs.cmu.edu/~avrim/ML10/lect0125.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Process of Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1^{o}.\\space Start\\space with\\space \\overrightarrow{w_{0}}=\\overrightarrow{0}\\in R^{d},\\space t=1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2^{o}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\\\\n",
    "while(1)\\space for\\space i=1,...,N\n",
    "\\\\\n",
    "\\{\\space if\\space y_{i}(w^{T}X_{i})<0,\\space\\text{%if your classifier does wrong on the ith sample}\\\\\n",
    "\\hat{w_{t}}=\\hat{w_{t-1}}+y_{i}X_{i},\\space\\text{%$y_i=\\pm 1$}\n",
    "\\\\\n",
    "t++ \\space\\}\\\\\n",
    "\\text{if we don't find any violation break;}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of The Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with the all-zeroes weight vector $w_1$ = 0, and initialize t to 1. Also let’s automatically scale all examples x to have (Euclidean) length 1, since this doesn’t affect which side of the plane they are on.\n",
    "\n",
    "2. Given example X, predict positive iff $w_t·x > 0$.\n",
    "\n",
    "3. On a mistake, update as follows:\n",
    "\n",
    "    - Mistake on positive: $w_t+1 ← w_t + x$;\n",
    "\n",
    "    - Mistake on negative: $w_t+1 ← w_t − x$;\n",
    "\n",
    "4. t ← t + 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{cases}\n",
    "\\frac{1}{n}\\sum\\limits_{1}^{n}(y_{i}-w^{T}X_{i})^2,\\space for\\space linear\\space regression\n",
    "\\\\\n",
    "\\frac{1}{N}\\sum\\limits_{1}^{N}\\max\\{0,-y_{i}w^{T}X_{i}\\}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 1(If margin $\\gamma$ is bounded, can perceptron algorithm converge?):\n",
    "\n",
    "Let S be a sequence of labeled examples consistent with a linear threshold function $w^*·x$ > 0, where $w^*$ is a unit-length vector. Then the number of mistakes M on S made by the online Perceptron algorithm is at most $(1/\\gamma)^2$, where\n",
    "\n",
    "$\\gamma = \\underset{X\\in S}{min}\\frac{|w^*X|}{||X||}$\n",
    "\n",
    "\n",
    "(I.e., if we scale examples to have Euclidean length 1, then $\\gamma$ is the minimum distance of any example to the plane $w^∗·x$ = 0.)\n",
    "\n",
    "The parameter $“\\gamma”$ is often called the “__margin__” of $w^∗$ (or more formally, the L2 margin because we are scaling by the $L_2$ lengths of the target and examples). Another way to view the quantity $\\frac{|w^∗X|}{||X||}$ is that it is the __cosine__ of the __angle__ between __X__ and __$w^*$__, so we will also use $cos(w^∗,X)$ for it.\n",
    "\n",
    "__Assume__:$\\|w^*\\|\\leq 1,\\space\\forall i, \\|X_i\\|_{2}\\leq \\Gamma$\n",
    "\n",
    "__What we have__: $\\gamma > 0,\\exists \\space classifier\\space w\\space, w^*: \\text{best classifier,final target we want to get;}$\n",
    "\n",
    "__Need to prove__: $\\forall i,\\space y_{i}(w_{i}^{T}X_{i})>\\gamma,\\underset{?}{\\Rightarrow} T<\\frac{constant}{\\gamma^2}$, without $\\Delta$ (step length), can the Perceptron Algprithm converge?\n",
    "\n",
    "__Proof of Theorem 1__:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Claim 1__: $w_{t+1}·w^* \\geq w_t·w^∗ + \\gamma$. That is, every time we make a mistake, the dot-product\n",
    "of our weight vector with the target increases by at least $\\gamma$.\n",
    "\n",
    "\n",
    "__Claim 2__: $||w_{t+1}||^2 \\leq ||w_t||^2 + 1$, suppose $||X||=1$.That is, every time we make a mistake, the length squared of our weight vector increases by at most 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For Claim 1__:\n",
    "\\begin{equation}\n",
    "<w^{*},w_{t}>=w^{*T}w_{t}=<w^{*},w_{t-1}+y_{i}X_{i}>\n",
    "\\\\\n",
    "= <w^{*},w_{t-1}>+<w^*,y_{i}X_{i}>\\space \\geq\\space <w^{*},w_{t－1}>+\\gamma\n",
    "\\\\\n",
    "since\\space w^{*},\\forall\\space y_{i}(w^{*}X_{i})>\\gamma\n",
    "\\\\\n",
    "\\Rightarrow <w^{*},w_{t}>\\space\\geq\\space <w^{*},w^{*}_{t－1}>+\\gamma\n",
    "\\\\\n",
    "\\text{means: each time the classifier $w_t$ makes a mistake}\\\\\n",
    "\\text{the dot-product of our weight vector with the target,which is $w^*$, increases by at least $\\gamma$}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "__Q.E.D__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Claim 1 implies__:\n",
    "\\begin{equation}\n",
    "<w^{*},w_{t}>\\space\\geq\\space <w^{*},w_{0}>+T\\space\\gamma, \\text{since $w_0=\\overrightarrow{0}$}\\\\\n",
    "\\Rightarrow <w^{*},w_{t}>\\space\\geq\\space T\\space\\gamma\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For Claim 2__:\n",
    "\\begin{equation}\n",
    "<w_{t},w_{t}>\\space =\\space<w_{t-1}+y_{i}X_{i},w_{t-1}+y_{i}X_{i}>\n",
    "\\\\\n",
    "=<w_{t-1},w_{t-1}>+\\underset{incorrectly\\space classified\\space <\\space 0}{\\underline{2y<w_{t-1},X_{i}>}}+<X_{i},X_{i}>\n",
    "\\\\\n",
    "since\\space y=\\begin{cases} +1 \\\\ -1 \\end{cases} \\Rightarrow \\|y\\|=1\n",
    "\\\\\n",
    "\\Rightarrow\\space <w_{t},w_{t}>\\space\\leq\\space <w_{t-1},w_{t-1}>+\\|X_i\\|_{2}^{2},\\Rightarrow\\text{ claim 2 Q.E.D}\n",
    "\\\\\n",
    "\\leq \\|w_{t-1}\\|_{2}^{2}+\\Gamma^2\n",
    "\\\\\n",
    "\\leq ||w_0||_2^2+\\space T\\Gamma^2 = T\\Gamma^2\n",
    "\\\\\n",
    "\\Rightarrow ||w_t||_2^2\\leq T\\Gamma^2 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Claim 2 implies__:\n",
    "\n",
    "After T mistakes,\n",
    "\n",
    "\\begin{equation}\n",
    "||w_t|| \\leq \\sqrt{T}\\Gamma ...(1)\\\\\n",
    "\\text{since }\\|w^*\\|\\leq 1 \\Rightarrow ||w_t|| \\geq w_tw^*...(2)\\\\\n",
    "\\text{since }<w^{*},w_{t}>\\space\\geq\\space T\\space\\gamma...(3)\\\\\n",
    "\\text{By (1),(2),(3)} \\Rightarrow \\sqrt{T}\\Gamma \\geq\\space T\\space\\gamma\\\\\n",
    "\\Rightarrow T \\leq \\frac{\\Gamma^2}{\\gamma^2} \\Rightarrow converge\\\\\n",
    "\\text{which means after at most making int($\\frac{\\Gamma^2}{\\gamma^2}$)+1 mistakes, we can find the optimal $w^*$}\\\\\n",
    "\\text{Theorem 1} \\rightarrow Q.E.D\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If data is separable by a large margin, then Perceptron is a good algorithm to use.\n",
    "\n",
    "- What if there is no perfect separator? Which means only most of the data is separable by a large margin, or what if $w^∗$ is not perfect? \n",
    "\n",
    "## $\\Downarrow$\n",
    "\n",
    "Find the bound of the total number of mistakes,i.e __T__, we can make in terms of the total distance between the not perfectly seperated points to the line with maximum margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define this distance as:\n",
    "\n",
    "### $D_{\\gamma}$\n",
    "\n",
    "Now, after making __T__ mistakes, \n",
    "\n",
    "1) by __Claim 1__, we have:\n",
    "\n",
    "### $w_t.w^*\\leq T\\gamma - D_{\\gamma}$\n",
    "\n",
    "2) by __Claim 2__, we have:\n",
    "\n",
    "### \\begin{equation}\n",
    "\\sqrt{T}\\Gamma \\geq\\space T\\space\\gamma - D_{\\gamma}\\\\\n",
    "\\text{Define$\\frac{1}{\\gamma}D_{\\gamma}$ as total hinge-loss of $w^2$}\\\\\n",
    "\\text{we can find the bound of T}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__:\n",
    "\n",
    "Finding an approximately optimal separator is __NP-hard__, although we find a way to express time consuming in terms of “total distance” parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Margin Perceptron Algorithm\n",
    "\n",
    "__Two Method__ to find __maximum margin__ seperator:\n",
    "\n",
    "1) SVM\n",
    "\n",
    "2) Perceptron -> __Approximately__ maximize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Function: \n",
    "### $y=logit(x)=\\frac{1}{1+e^x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\rightarrow$ try to regress, $X_{variable,\\space vector}\\in R$, and $Y_{label}=[0,1]\\space \\leftarrow$ Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\rightarrow$ classifier function:\n",
    "### \\begin{equation}\n",
    "\\begin{cases}\n",
    "P(x)=P(y=1|X)=\\frac{1}{1+e^{-w^{T}X}}\n",
    "\\\\\n",
    "1-P(x)=p(y=0|X)=1-P(y=1|X)=\\frac{e^{-w^{T}X}}{1+e^{-w^{T}X}}\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, use MLE to get w:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\rightarrow$ for single sample\n",
    "### \\begin{equation}\n",
    "P(Y|w,X)=\\prod\\limits_{1}^{N}{P(X_i)^{y_i}[1-P(X_i)^{1-y_i}]}\n",
    "\\\\\n",
    "w=arg\\max\\limits_{w}P(Y|w,X)=\\prod\\limits_{i=1}^{N}[P(X_i)^{y_i}(1-P(X_i)^{1-y_i})]\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
