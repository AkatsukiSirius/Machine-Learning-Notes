{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 0. Examples of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the assocation between the products customer buys.\n",
    "\n",
    "__Conditional Probability__: P(Y|X), ex. P(chips|beer) = 0.7; P(Y|X,D), D can be attributes of customer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creadit Scoring__: bank, risk ~ customer;\n",
    "\n",
    "Class1, low - risk; Class 2, hig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Discriminant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __function__ that __seperate__ the example of __different classes__.\n",
    "\n",
    "__Example__:\n",
    "\n",
    "IF __income__ > $\\theta_1$ __and savings__ > $\\theta_2$ THEN __low-risk__ ELSE __high-risk__\n",
    "\n",
    "- To predict the category of customers.\n",
    "\n",
    "- In some cases, instead of making a 0/1 (low-risk/high-risk) type decision, we may want to calculate a probability, namely, P(Y|X).\n",
    "\n",
    "    Y: 0 or 1; X: customer attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Optical Character Recognition - hand writting\n",
    "\n",
    "2. Face Recognition\n",
    "\n",
    "3. Medical Diagnosis\n",
    "\n",
    "4. Speech recognition\n",
    "\n",
    "__Knowledge Extraction__: Learning a rule from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Response Surface Design__: Fit a regression model linking inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ranking Function__: Sometimes instead of estimating an absolute numeric value, we want to be able to __learn relative positions__. For example, in a __recommendation system__ for movies, we want to generate a list ordered by how much we believe the user is likely to enjoy each. Depending on the __movie attributes__ such as genre, actors, and so on, and using the ratings of the user he/she has already seen, we would like to be able to __learn__ a __ranking function__ that we can then use to choose among new movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only have input, no correct output(label) being provided. The __aim__ is to find the __regularities__ in input.\n",
    "\n",
    "There is a structure to the input space such that certain patterns occur more often than others, and we want to see what generally happens and what does not. In statistics, tis is called __density estimation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Clustering\n",
    "\n",
    "One method for density estimation is __clustering__ where the aim is to find clusters or grouping of output.\n",
    "\n",
    "__Example__: __Customer Segmentation__\n",
    "\n",
    "- company with data of past customers, contains demographic information and past transactions.\n",
    "\n",
    "- Customer Segmentation. In this case, a clustering model allocates customers simiar in their attributes to the same group, providing the company with natural groupings of its customers. \n",
    "\n",
    "\n",
    "__ Example__: __Image Compression__\n",
    "\n",
    "- Groups pixels with similar colors and the frequency of corresponding color group.\n",
    "\n",
    "__Example__: __Document Clustering__\n",
    "\n",
    "\n",
    "__Example__: __Bioinformation__\n",
    "\n",
    "- sequence matching, aligment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reinforcement Learning\n",
    "\n",
    "__Actions__: Output of system is a sequence of actions.\n",
    "\n",
    "__Policy__: Sequence of correct actions to reach the goal.\n",
    "\n",
    "__Reinforcement Learning__: There is no such thing as the best action in any intermediate state; an __action__ is\n",
    "__good__ if it is __part__ of a __good policy__. In such a case, the machine learning program should be able to assess the goodness of policies and __learn from past good action sequences__ to be able to __generate__ a __policy__.\n",
    "\n",
    "__Example__: __Chess__\n",
    "\n",
    "__Example__: __Robot Navigating__\n",
    "\n",
    "- Search a goal location in enviornment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Notes\n",
    "\n",
    "In statistics, going from __particular observations__ to __general descriptions__ is called __inference__ and learning is called __estimation__.\n",
    "\n",
    "In statistics: __Classification__ ~ __Discriminant Analysis__ \n",
    "\n",
    "In engineering: __Classification__ ~ __Pattern Recognition__ \n",
    "\n",
    "HMM -> Speech Recognition\n",
    "\n",
    "__SVM__: is a Kernel - based algorithm, through kernel functions can be adapted to various applications, especially in bioinformatics and language processing.\n",
    "\n",
    "__General Models__ ~ __Bayesian__ : Use of generative models that explain the observed data through the interaction of a set of __hidden factors__. Generally, __graphical models__ are used to __visualize__ the __interaction__ of the __factors__ and the __data__, and __Bayesian formalism__ allows us to define our prior information on the hidden factors and the model, as well as to infer the model parameters.\n",
    "\n",
    "In CS Databases: __Data Mining__ ~ __Knowledge Discovery__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example -> Seperate car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Learning object: family car;\n",
    "\n",
    "- Positive Examples: People lable as family cars.\n",
    "\n",
    "- Negative Example: People lable as non family cars.\n",
    "\n",
    "2) Goal: predict whether a car is a family car.\n",
    "\n",
    "3) Input: the attributes we want to research on.\n",
    "\n",
    "- In this case, 1st input $x_1$ price, 2nd input, $x_2$ price, engine volume (cubic centimeters).\n",
    "\n",
    "## $x = \\begin{bmatrix} \n",
    "x_1\\\\\n",
    "x_2 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "4) Lables: denote the category(type).\n",
    "\n",
    "## $r = \n",
    "\\begin{cases}\n",
    "    1,& \\text{if  x is a positive example}\\\\\n",
    "    0,              & \\text{if x is a negative example}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "5) Training Set\n",
    "\n",
    "- (x,r): each car is represented by such an ordered pair.\n",
    "\n",
    "## $x\\space = \\{x^t,r^t\\}^N_{t = 1}$\n",
    "\n",
    "6) Discriminant\n",
    "\n",
    "#### $(p_1 \\leq price\\space \\leq p_2) AND (e_1 \\leq engine\\space power \\leq e_2)$\n",
    "\n",
    "7) Hypothesis Class\n",
    "\n",
    "Defined in 6)\n",
    "\n",
    "8) Learning Algorithm\n",
    "\n",
    "Find particular hypothesis, h $\\in$ H, specified by a particular quadruple of $(p_1^h,p_2^h,e_1^h,e_2^h)$, to approximate C as closely as possible.\n",
    "\n",
    "Find the four parameter $(p_1^h,p_2^h,e_1^h,e_2^h)$ which seperate the samples perfectly.\n",
    "\n",
    "9) Goal (Algorithm)\n",
    "\n",
    "Find h $\\in$ H, such that h($x_{unknown}$) matches C($x_{unknown}$).\n",
    "\n",
    "## $h(x) = \n",
    "\\begin{cases}\n",
    "    1,& \\text{if  x is a positive example}\\\\\n",
    "    0,              & \\text{if x is a negative example}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "10) Error\n",
    "\n",
    "- __Empirical Error__: The __empirical error__ is the __proportion__ of __training instances__ where __predictions__ of __h__ do not match the required values given in __X__. The __error__ of hypothesis h given the training set X is\n",
    "\n",
    "\n",
    "### $E(h|X)=\\sum_{i=1}^{N}1(h(x^t)\\neq r^t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generalization\n",
    "- How well our hypothesis will correctly classify future examples that are not part of the training set.\n",
    "\n",
    "### 2.2 Most Specific Hypothesis , S < actual class, C < General Hypothesis, G\n",
    "\n",
    "### 2.3 Version space: \n",
    "\n",
    "- Made up by any __h__ $\\in$ __H__ between S and G is a valid hypothesis with no error. Which means __h__ is consistent with the training set.\n",
    "\n",
    "### 2.4 Margin\n",
    "\n",
    "- Distance between the boundary and the instances closest to it.\n",
    "\n",
    "### 2.5 VC Dimension\n",
    "\n",
    "- __Problem__: __N points__ separate to __two group__, there are __2^N__ ways to label them. Therefore, __2^N different learning problems__ can be __defined__ by __N data points__. If we can find a hypothesis __h $\\in$ H__ that separates the PE from NE, then we say H shatters N points. That is, any learning problem definable by N examples can be learned with no error by a hypothesis drawn from H.\n",
    "\n",
    "- __VC Dimension:__ The maximum number of points that can be shattered by __H__, denotated as VC(__H__), measures the capacity of __H__.\n",
    "\n",
    "- __Note__: \n",
    "hypothesis classes with small VC dimensions are applicable and are preferred over those with large VC dimensions, for example, a lookup table that has infinite VC dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
